{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a custom solver\n",
    "\n",
    "Now that we have all the elements, we can write a (very basic) solver in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Initialization (see \"00 Basic solver usage\").\"\"\"\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Silence caffe network loading output. Must be set before importing caffe\n",
    "os.environ[\"GLOG_minloglevel\"] = '2'\n",
    "import caffe\n",
    "CAFFE_ROOT=\"/caffe\"\n",
    "os.chdir(CAFFE_ROOT) # change the current directory to the caffe root, to help\n",
    "                     # with the relative paths\n",
    "USE_GPU = True\n",
    "if USE_GPU:\n",
    "    caffe.set_device(0)\n",
    "    caffe.set_mode_gpu()\n",
    "else:\n",
    "    caffe.set_mode_cpu()\n",
    "# For reproducible results\n",
    "caffe.set_random_seed(0) # custom modification, remove this line from your code if it doesn't work\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Loading the data\n",
    "\n",
    "We are going to load the data from an LMDB database with the python `lmdb` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lmdb\n",
    "\n",
    "def image_generator(db_path):\n",
    "    \"\"\"A generator that yields all the images in the database, normalized.\"\"\"\n",
    "    db_handle = lmdb.open(db_path, readonly=True) # We don't need to write in there\n",
    "    with db_handle.begin() as db:\n",
    "        cur = db.cursor() # Points to an element in the database\n",
    "        for _, value in cur: # Iterate over all the images\n",
    "            # Read the LMDB and transform the protobuf into a numpy array\n",
    "            datum = caffe.proto.caffe_pb2.Datum()\n",
    "            datum.ParseFromString(value) # String -> Protobuf\n",
    "            int_x = caffe.io.datum_to_array(datum) # parse the datum into a nparray\n",
    "            x = np.asfarray(int_x, dtype=np.float32) # Convert to float\n",
    "            yield x - 128 # Normalize by removing the mean\n",
    "            \n",
    "def batch_generator(shape, db_path):\n",
    "    \"\"\"A generator that yield all the images in the database by batches\"\"\"\n",
    "    gen = image_generator(db_path)\n",
    "    res = np.zeros(shape) # Result array\n",
    "    while True: # It will stop when next(gen) finishes\n",
    "        for i in range(shape[0]):\n",
    "            res[i] = next(gen) # Set by slices\n",
    "        yield res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the network\n",
    "\n",
    "Every so often during training, we will test the network. It is a simple matter of iterating over the whole test database, running it through the network and collecting the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_network(test_net, db_path_test):\n",
    "    # Average the accuracy and loss over the number of batches\n",
    "    accuracy = 0\n",
    "    loss = 0\n",
    "    test_batches = 0\n",
    "    input_shape = test_net.blobs[\"data\"].data.shape\n",
    "    for test_batch in batch_generator(input_shape, db_path_test):\n",
    "        test_batches += 1\n",
    "        # Run the forward step\n",
    "        test_net.blobs[\"data\"].data[...] = test_batch\n",
    "        test_net.forward()\n",
    "        # Collect the outputs\n",
    "        accuracy += test_net.blobs[\"accuracy\"].data\n",
    "        loss += test_net.blobs[\"loss\"].data\n",
    "    return (accuracy / test_batches, loss / test_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the network\n",
    "\n",
    "Here, we want two networks: the training network, and the testing one. However, they should share the weights of their layers. Fortunately, `pycaffe` provides a `share_with` method to do just that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net_path = \"examples/mnist/lenet_train_test.prototxt\"\n",
    "net = caffe.Net(net_path, caffe.TRAIN)\n",
    "test_net = caffe.Net(net_path, caffe.TEST) # Testing version\n",
    "net.share_with(test_net) # Share the weights between the two networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving the network\n",
    "\n",
    "Now is time to do the actual solving. This solver is relatively minimalistic, while still presenting the main features you could expect from a solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "Iter 50: loss=1.38813483715\n",
      "Iter 100: loss=0.689562916756\n",
      "Iter 150: loss=0.589528501034\n",
      "Iter 200: loss=0.59515106678\n",
      "Testing network: accuracy=0.881399998069, loss=0.428480608463\n",
      "Iter 250: loss=0.606553435326\n",
      "Iter 300: loss=0.395008981228\n",
      "Iter 350: loss=0.173302009702\n",
      "Iter 400: loss=0.336296796799\n",
      "Testing network: accuracy=0.91219999969, loss=0.309457472917\n",
      "Iter 450: loss=0.210273712873\n",
      "Iter 500: loss=0.312768518925\n",
      "Iter 550: loss=0.262531369925\n",
      "Iter 600: loss=0.321408331394\n",
      "Testing network: accuracy=0.922200001478, loss=0.266542272568\n",
      "Iter 650: loss=0.384231954813\n",
      "Iter 700: loss=0.22680439055\n",
      "Iter 750: loss=0.241274178028\n",
      "Iter 800: loss=0.288684517145\n",
      "Testing network: accuracy=0.937700000405, loss=0.222113073906\n",
      "Iter 850: loss=0.224339067936\n",
      "Iter 900: loss=0.139535531402\n",
      "Starting epoch 1\n",
      "Iter 950: loss=0.22338809073\n",
      "Iter 1000: loss=0.129087716341\n",
      "Testing network: accuracy=0.94470000267, loss=0.193666659808\n",
      "Iter 1050: loss=0.261462211609\n",
      "Iter 1100: loss=0.106695577502\n",
      "Iter 1150: loss=0.0788200199604\n",
      "Iter 1200: loss=0.250188231468\n",
      "Testing network: accuracy=0.952200003862, loss=0.172415709603\n",
      "Iter 1250: loss=0.130198031664\n",
      "Iter 1300: loss=0.157033234835\n",
      "Iter 1350: loss=0.504559338093\n",
      "Iter 1400: loss=0.106351479888\n",
      "Testing network: accuracy=0.95490000248, loss=0.160248942492\n",
      "Iter 1450: loss=0.259181439877\n",
      "Iter 1500: loss=0.152146711946\n",
      "Iter 1550: loss=0.14323566854\n",
      "Iter 1600: loss=0.194749563932\n",
      "Testing network: accuracy=0.955800002217, loss=0.156148537248\n",
      "Iter 1650: loss=0.138044655323\n",
      "Iter 1700: loss=0.0528031140566\n",
      "Iter 1750: loss=0.275783896446\n",
      "Iter 1800: loss=0.148613870144\n",
      "Testing network: accuracy=0.95930000484, loss=0.145645902357\n",
      "Iter 1850: loss=0.0539376437664\n",
      "Training finished after 1874 iterations\n",
      "Final performance: accuracy=0.958600003123, loss=0.140253234301\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2 # How many times we are going to run through the database\n",
    "iter_num = 0 # Current iteration number\n",
    "\n",
    "# Training and testing examples\n",
    "db_path = \"examples/mnist/mnist_train_lmdb\"\n",
    "db_path_test = \"examples/mnist/mnist_test_lmdb\"\n",
    "\n",
    "# Learning rate. We are using the lr_policy \"inv\", here, with no momentum\n",
    "base_lr = 0.01\n",
    "# Parameters with which to update the learning rate\n",
    "gamma = 1e-4\n",
    "power = 0.75\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Starting epoch {}\".format(epoch))\n",
    "    # At each epoch, iterate over the whole database\n",
    "    input_shape = net.blobs[\"data\"].data.shape\n",
    "    for batch in batch_generator(input_shape, db_path):\n",
    "        iter_num += 1\n",
    "        \n",
    "        # Run the forward step\n",
    "        net.blobs[\"data\"].data[...] = batch\n",
    "        net.forward()\n",
    "        \n",
    "        # Clear the diffs, then run the backward step\n",
    "        for name, l in zip(net._layer_names, net.layers):\n",
    "            for b in l.blobs:\n",
    "                b.diff[...] = net.blob_loss_weights[name]\n",
    "        net.backward()\n",
    "        \n",
    "        # Update the learning rate, with the \"inv\" lr_policy\n",
    "        learning_rate = base_lr * math.pow(1 + gamma * iter_num, - power)\n",
    "        \n",
    "        # Apply the diffs, with the learning rate\n",
    "        for l in net.layers:\n",
    "            for b in l.blobs:\n",
    "                b.data[...] -= learning_rate * b.diff\n",
    "        \n",
    "        # Display the loss every 50 iterations\n",
    "        if iter_num % 50 == 0:\n",
    "            print(\"Iter {}: loss={}\".format(iter_num, net.blobs[\"loss\"].data))\n",
    "            \n",
    "        # Test the network every 200 iterations\n",
    "        if iter_num % 200 == 0:\n",
    "            print(\"Testing network: accuracy={}, loss={}\".format(*test_network(test_net, db_path_test)))\n",
    "\n",
    "print(\"Training finished after {} iterations\".format(iter_num))\n",
    "print(\"Final performance: accuracy={}, loss={}\".format(*test_network(test_net, db_path_test)))\n",
    "# Save the weights\n",
    "net.save(\"examples/mnist/lenet_iter_{}.caffemodel\".format(iter_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This minimalistic solver shows the possibilities of what you can do in Python. However, to make a proper solver, we would need to implement many more features: different learning rate policies (momentum), gradient clipping, batch normalization, and other potential improvements.\n",
    "\n",
    "One way to do that is to implement everything in Python, but it has its disadvantages:\n",
    "\n",
    "  - It is slower than C++, even though usually most of the time will be spent on the GPU anyway\n",
    "  - Re-writing it should not be necessary, the logic is already there in the C++ code\n",
    "  - Re-writing it **will** lead to bugs, sometimes hard to check\n",
    "  \n",
    "For all these reasons, sometimes it's better to just call the c++ functions directly. Caffe has a relatively limited python API for now, but it will grow, and it is relatively easy to modify to add the functionnalities you want. The next tutorial will cover Python API modifications for custom function calls."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
